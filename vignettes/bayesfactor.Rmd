---
title: "Bayes Factors"
output: 
  github_document:
    toc: true
    fig_width: 10.08
    fig_height: 6
  rmarkdown::html_vignette:
    toc: true
    fig_width: 10.08
    fig_height: 6
tags: [r, bayesian, bayesfactors]
vignette: >
  \usepackage[utf8]{inputenc}
  %\VignetteIndexEntry{Bayes Factor}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(5)
```

# Bayes Factors

Bayes factors are indices of *relative* evidence of one model over another, which are used in Bayesian inference as alternatives to classical (frequentists) hypothesis testing indices. In the Baesian framework, a Bayes factor can also be thought of as a quantity by which some *a prior* belief about the relative odds of two models are to be updated in light of the observed data.

According to Bayes' theorem:

$$
P(M|D) = \frac{P(D|M)P(M)}{P(D)}
$$

Then by comparing two models, we get:

$$
\frac{P(M_1|D)}{P(M_2|D)} = \frac{P(D|M_1)}{P(D|M_2)} \times \frac{P(M_1)}{P(M_2)}
$$
Where the middle term is the Bayes factor:
$$
BF_{12}=\frac{P(D|M_1)}{P(D|M_2)}
$$
Bayes factor are typically computed as the ratio of marginal likelihoods of two competing hypotheses / models, but was we will see, and is evident from the equation above, they can also be computed by dividing the posterior-odds by the prior-odds.

## Savage-Dickey density ratio Bayes factor

The Savage-Dickey density ratio can be used to answer the question:

> Given the data, is the null more, or less likely?

This is done by by comparing the density (likelihood) of the null value under the prior between the prior and posterior distributions (Read much much more [here](http://dx.doi.org/10.1016/j.cogpsych.2009.12.001)).

Let's use the Student's Sleep data, and try and answer the question: given the observed data, is it more or less likely that the drug has no effect?

```{r sleep_boxplot, echo=FALSE}
library(ggplot2)
ggplot(sleep, aes(group, extra)) +
  geom_boxplot() +
  # see::theme_modern()
  theme_light()
```


```{r rstanarm_disp, eval=FALSE}
library(rstanarm)

stan_fit <- stan_glm(extra ~ group, data = sleep, family = gaussian())
stan_fit
```

```{r rstanarm_fit, echo=FALSE, message=FALSE}

library(rstanarm)
stan_fit <- stan_glm(extra ~ group, data = sleep, family = gaussian())
# how to prevent all the printing????

stan_fit
```


We can use `as.matrix` or `as.data.frame` to extract the posterior of the `group2` parameter, and the `insight` package to see what the prior distribution used:
```{r prior_n_post}
group2_post <- as.matrix(stan_fit, pars = "group2")

insight::get_priors(stan_fit)

group2_prior <- rnorm(8000, mean = 0, sd = 5.044799)
```

```{r prior_n_post_plot, echo=FALSE}
f_post <- suppressWarnings(logspline::logspline(group2_post)) 
f_prior <- suppressWarnings(logspline::logspline(group2_prior)) 

x_lims <- range(c(group2_post,group2_prior)) * 0.7

ggplot() +
  aes(x = 0, y = 0) +
  stat_function(aes(color = "Posterior"), fun = function(x) logspline::dlogspline(x,f_post), xlim = x_lims,
                size = 1) +
  stat_function(aes(color = "Prior"), fun = function(x) logspline::dlogspline(x,f_prior), xlim = x_lims,
                size = 1) +
  labs(x = 'group2', y = 'density', color = '') + 
  theme_light() + 
  theme(legend.position = c(0.2,0.8)) + 
  NULL

```

Looking at the distributions, we can see that the posterior is no longer centered at 0, but at `r round(median(group2_post),2)`. But this does not mean that an effect of 0 is necessarily less likely. To do that, we will use `bayesfactor_savagedickey`!

```{r savagedickey}
library(bayestestR)

bayesfactor_savagedickey(posterior = group2_post, prior = group2_prior)
```

This BF indicates the the likelihood of an effect of size 0 is 0.98 less likely (or 1.02 = 1/0.98 more likely) under posterior. That means the though the center of distribution has shifted, the who posterior distribution as a whole is still quite dense around the null.

We can also conduct a directional test is we have some prior hypothesis about the direction of the effect:

```{r prior_n_post_plot_one_sided, echo=FALSE}
f_post <- suppressWarnings(logspline::logspline(group2_post[group2_post > 0], lbound = 0)) 
f_prior <- suppressWarnings(logspline::logspline(group2_prior[group2_prior > 0], lbound = 0)) 

x_lims <- c(0,max(c(group2_post,group2_prior))) * 0.7

ggplot() +
  aes(x = 0, y = 0) +
  stat_function(aes(color = "Posterior"), fun = function(x) logspline::dlogspline(x, f_post), xlim = x_lims, size = 1) +
  stat_function(aes(color = "Prior"), fun = function(x) logspline::dlogspline(x, f_prior), xlim = x_lims, size = 1) +
  labs(x = 'group2', y = 'density', color = '') + 
  theme_light() + 
  theme(legend.position = c(0.8,0.8)) + 
  NULL

```

```{r savagedickey_one_sided}
group2_sd <- bayesfactor_savagedickey(posterior = group2_post, prior = group2_prior, direction = ">")
group2_sd
```

As we can see, given that we have an a-prior assumption about the direction of the effect (as evident in the "halved" prior distribution), the null is now `r round(group2_sd$BFsd[1],1)` times more likely under the prior, indicating that given the observed data, the posterior mass has somewhat shifted away from the null value, giving evidence against the null!

(Note that a Bayes factor of `r round(group2_sd$BFsd[1],1)` is still considered quite week evidence.)

## Comparing fitted models

Bayes factors can also be used to compare models. In these cases they answer the question:

> Is the observed data more likely given Model 1, or Model 2?

This is usually done by computing the marginal likelihoods of two models. In such a case, the Bayes factor is a measure of relative evidence between the two compared models. Note that the compared models *do not* need to be nested models (see `brms2` and `brms3` below).

### `brms` and `rstanarm`

Let's first fit some Bayesian models with `brms`:

```{r brms_disp, eval=FALSE}
library(brms)

brms1 <- brm(Sepal.Length ~ 1, data = iris, save_all_pars = TRUE)
brms2 <- brm(Sepal.Length ~ Species, data = iris, save_all_pars = TRUE)
brms3 <- brm(Sepal.Length ~ Petal.Length, data = iris, save_all_pars = TRUE)
brms4 <- brm(Sepal.Length ~ Species + Petal.Length, data = iris, save_all_pars = TRUE)
brms5 <- brm(Sepal.Length ~ Species * Petal.Length, data = iris, save_all_pars = TRUE)

```

We can now compare these models with the `bayesfactor_models` function, using the `denomanator` argument to specify which model all models will be compared to:

```{r brms_models_disp, eval=FALSE}
brms_models <- bayesfactor_models(brms2, brms3, brms4, brms5, denominator = brms1)
brms_models
```


```{r brms_models}
brms_models <-
  structure(
    list(
      Model = c(
        "Species",
        "Petal.Length",
        "Species + Petal.Length",
        "Species * Petal.Length",
        "1"
      ),
      log.BF = c(
        68.508589034605,
        102.554331682822,
        128.609564577131,
        128.872267410808,
        0
      )
    ),
    class = c("bayesfactor_models",
              "data.frame"),
    row.names = c(NA, -5L),
    denominator = 5L,
    BF_method = "marginal likelihoods (bridgesampling)"
  )
brms_models
```


We can see that the full model is the best model - with $BF_{\text{m0}}=9\times 10^{55}$ compared to the null (intercept only). We can also change the reference model to the man effect model:

```{r}
update(brms_models, reference = 3)
```

As we can see, though the full model is the best, there hardly any evidence it is preferable to the main effects model.

Notes:  
  
  - `brmsfit` models **must** have been fitted with `save_all_pars = TRUE`
  - `stanreg` models **must** have been fitted with a defined `diagnostic_file`.

### All other models: BIC approximation

Surprisingly enough, we can compute Bayes factors and compare non-Bayesian models! This is done by comparing BIC measures, and also allows for comparing non-nested models (read more [here](https://doi.org/10.3758/BF03194105).


```{r lme4, message=FALSE}
library(lme4)
mo1 <- lmer(Sepal.Length ~ (1 | Species), data = iris)
mo2 <- lmer(Sepal.Length ~ Petal.Length + (1 | Species), data = iris)
mo3 <- lmer(Sepal.Length ~ Petal.Length + (Petal.Length | Species), data = iris)
mo4 <- lmer(Sepal.Length ~ Petal.Length + Petal.Width + (Petal.Length | Species), data = iris)

bayesfactor_models(mo1, mo2, mo3, mo4, denominator = 1L)
```


## Inclusion Bayes factors via Bayesian model averaging

Inclusion Bayes factors answer the question:

> Given the observed data, how much more likely have models with a particular effect become, compared to models without that particular effect?

In other words, on average - are models with effect $X$ better than models without effect $X$?

Lets use the `brms` example from above:

```{r inclusion_brms}
bayesfactor_inclusion(brms_models)
```

As we can see, across all 5 models, the addition of each effect term makes the model on average, more likely.

We can also compare only matched models - i.e., a model without effect $A$ will only be compared to models *with* effect $A$, but not with models with higher-level interaction. (See explanation for why you might want to do this [here](https://www.cogsci.nl/blog/interpreting-bayesian-repeated-measures-in-jasp).)

```{r inclusion_brms2}
bayesfactor_inclusion(brms_models, match_models = TRUE)
```

In this case, it did not change the inclusion Bayes factors by much (by did change the prior and posterior effect probabilies).

### Comparison with JASP

`bayesfactor_inclusion` is meant to provide Bayes Factors across model averages, similar to JASP's *Effects* option. Lets compare the two:

#### Compared across all models

```{r JASP_all, message=FALSE}
library(BayesFactor)

ToothGrowth$dose <- as.factor(ToothGrowth$dose)

BF_ToothGrowth <- anovaBF(len ~ dose*supp, ToothGrowth)

bayesfactor_inclusion(BF_ToothGrowth)
```
![](JASP1.PNG)

#### Compared across matched models

```{r JASP_matched}
bayesfactor_inclusion(BF_ToothGrowth, match_models = TRUE)
```


![](JASP2.PNG)

#### With Nuisance Effects

We'll add `dose` to the null model in JASP, and do the same in `R`:

```{r JASP_Nuisance}
BF_ToothGrowth_against_dose <- BF_ToothGrowth[3:4]/BF_ToothGrowth[2]
BF_ToothGrowth_against_dose

# OR:
# update(bayesfactor_models(BF_ToothGrowth), subset = c(4,5), reference = 3)

bayesfactor_inclusion(BF_ToothGrowth_against_dose)
```

![](JASP3.PNG)

